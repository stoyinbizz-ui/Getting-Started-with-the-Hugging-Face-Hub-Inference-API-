{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stoyinbizz-ui/Getting-Started-with-the-Hugging-Face-Hub-Inference-API-/blob/main/Getting_Started_with_the_HuggingFace_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Getting Started with the Hugging Face Hub (Inference API)**\n",
        "\n",
        "* Heard of AI chat-based interfaces like **ChatGPT**, **Gemini**, **HuggingChat**?\n",
        "\n",
        "**What exactly is a Transformer-based language model?**\n",
        "\n",
        "These are large language models (LLMs) trained on massive datasets to understand and generate natural language. They are:\n",
        "\n",
        "* **Generative** â€” able to produce text and other content.\n",
        "* **Pre-trained** â€” trained in advance on large corpora.\n",
        "* **Transformer-based** â€” built on the transformer architecture that converts input to context-aware output.\n",
        "\n",
        "These models power many common NLP tasks: answering questions, summarizing content, translating languages, and generating human-like dialogue."
      ],
      "metadata": {
        "id": "cVuAgNVgtl_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Adding Your API Key to Colab**\n",
        "\n",
        "1. In Colab, click **ðŸ”‘ Secrets** in the left panel.\n",
        "2. Add a new secret with:\n",
        "\n",
        "   * **Name**: `HF_TOKEN`\n",
        "   * **Value**: *your API key*\n",
        "3. Grant notebook access to that secret.\n",
        "\n",
        "### **Loading the API Key in Your Code**\n",
        "\n",
        "First, retrieve the key securely:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F0aIQc1r74SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "PL6WGYuftpe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_API_KEY=userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "hoFzueaf31vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Then pass it to the SDK:**\n",
        "\n",
        "```python\n",
        "import os\n",
        "os.environ[\"HF_API_KEY\"] = HF_API_KEY\n",
        "\n",
        "from huggingface_hub import InferenceClient\n",
        "client = InferenceClient(token=os.environ[\"HF_API_KEY\"])\n",
        "```\n",
        "\n",
        "- <font color=\"red\">Warning</font>: Ensure that there are no whitespaces in your API key."
      ],
      "metadata": {
        "id": "xQlgleJw80CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install the Hugging Face Hub SDK**\n",
        "\n",
        "Now that your account and access token are ready, the next step is setting up your local environment. Weâ€™ll access models, datasets, and repos via the Hugging Face Hub Python library.\n",
        "\n",
        "You can install it using pip using the command below:\n"
      ],
      "metadata": {
        "id": "fZbXkk0wmHoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl80ZwM34HyZ",
        "outputId": "a9c0dd6f-a02b-433b-ffb4-ef43b66c390b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import packages**\n",
        "Import the necessary packages."
      ],
      "metadata": {
        "id": "AJeiBWD0nme6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import openai\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "5E_hD-onmLCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_API_KEY\"] = HF_API_KEY"
      ],
      "metadata": {
        "id": "ti0eRGvV8ntD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Instantiate a client**\n",
        "\n",
        "We will now create a client that can access various types of models globally. Please note that you should only provide an API key for authentication whenever you are initialising a client."
      ],
      "metadata": {
        "id": "eTaBwDF6nw9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "import os\n",
        "\n",
        "# Create client\n",
        "client = InferenceClient(token=os.environ[\"HF_API_KEY\"])\n",
        "\n",
        "prompt = \"write a poem about generative AI and it founders.\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "response = client.chat_completion(\n",
        "    messages=messages,\n",
        "    temperature=0.1,\n",
        "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mGBg9416KAW",
        "outputId": "38da8b2f-03b1-4914-cdc2-7c3e4a1ca9f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In silicon halls, a dream took shape,\n",
            "A future born of code and fate,\n",
            "Generative AI, a wondrous thing,\n",
            " Created by minds that dared to sing.\n",
            "\n",
            "Ian Goodfellow, a pioneer true,\n",
            "Invented GANs, a breakthrough anew,\n",
            "A neural network that could create and play,\n",
            "Unleashing art, in a digital way.\n",
            "\n",
            "Andrew Ng, a visionary guide,\n",
            " Led the charge, with a gentle stride,\n",
            "Deep learning's power, he did unfold,\n",
            "A new frontier, where AI could hold.\n",
            "\n",
            "Yoshua Bengio, a mastermind,\n",
            " Contributed greatly, to the design,\n",
            "Recurrent neural networks, a key to unlock,\n",
            "The secrets of language, and the human stock.\n",
            "\n",
            "Geoffrey Hinton, a giant in the field,\n",
            "His work on backpropagation, a story to yield,\n",
            "A fundamental concept, that paved the way,\n",
            "For the AI revolution, in a brighter day.\n",
            "\n",
            "These founders, of a new era's birth,\n",
            "Their work and passion, gave AI its mirth,\n",
            "Generative AI, a tool of great might,\n",
            "Creating art, music, and a digital light.\n",
            "\n",
            "Their legacy lives on, in the code they wrote,\n",
            "A testament to human ingenuity, and the power of thought,\n",
            "As AI continues to evolve, and grow,\n",
            "We owe a debt, to these pioneers, who showed the way to go.\n",
            "\n",
            "So let us celebrate, these visionaries true,\n",
            "Who brought us generative AI, and all it can do,\n",
            "Their contributions, a gift to us all,\n",
            "A future bright, where AI stands tall.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat_completion(\n",
        "    messages=messages,\n",
        "    temperature=0.7,\n",
        "    model=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    max_tokens=100\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFrrkzEN7Qp7",
        "outputId": "69e40e78-07e1-4c0c-9fc0-9cfac3919c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat_completion(\n",
        "    messages=messages,\n",
        "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZJEB6hz8M34",
        "outputId": "42f03f30-a925-48d5-e118-ecab02cd2523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It was a typical Tuesday morning in Silicon Valley when Elon Musk decided to shake things up. He had been feeling particularly mischievous that day, and as he scrolled through his Twitter feed, he spotted the perfect targets.\n",
            "\n",
            "First, he tweeted a cryptic message about his latest Neuralink project, claiming that it would allow people to control their dreams remotely. But just as people were getting excited, he dropped the bombshell: \"Just kidding, it's just a toaster with a fancy name.\"\n",
            "\n",
            "The tech community was left scratching their heads, but Musk wasn't done yet. Next, he announced that SpaceX would be launching a new rocket, dubbed the \"Hyperion,\" which would supposedly reach the moon in under an hour. But when asked for more details, he simply responded with a selfie of himself holding a rubber chicken, captioned \"Hyperion: coming soon to a moon near you.\"\n",
            "\n",
            "But Musk's piÃ¨ce de rÃ©sistance was yet to come. He started responding to people's tweets with absurd, over-the-top comments, like \"I'm secretly a time-traveling robot from the year 3050\" or \"I've been living on Mars for the past decade and I just forgot to tell anyone.\"\n",
            "\n",
            "The internet was in chaos, with people trying to figure out whether Musk was serious or just trolling them mercilessly. Some were outraged, while others were laughing along with him. But one thing was certain: Elon Musk was the biggest troll in the game.\n",
            "\n",
            "As the day went on, Musk's tweets became increasingly outlandish, with him claiming to have invented a machine that turned thoughts into reality, and another one that allowed him to communicate with aliens. But just when people thought he'd gone too far, he dropped another bombshell: \"Just kidding, guys. I'm actually just a normal dude from South Africa who loves playing pranks on the internet.\"\n",
            "\n",
            "The internet was left stunned, unsure of what to make of Musk's antics. But one thing was certain: Elon Musk was the king of trolling, and he would stop at nothing to keep the internet on its toes.\n",
            "\n",
            "As the sun set on Silicon Valley, Musk tweeted one final message: \"Just wanted to say thanks for playing along, guys. You're the best trolling community in the world. Don't worry, I'll be back tomorrow with more pranks.\" And with that, he disappeared into the night, ready to plan his next move in the never-ending game of trolling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What models can be used with the Python SDK?**\n",
        "\n",
        "Now you're ready to call models via the Hugging Face Inference API. Before generating responses, letâ€™s explore the models available for use with the SDK.\n",
        "\n",
        "For a more holistic view of available models, see the [Hugging Face Model Hub](https://huggingface.co/models).\n"
      ],
      "metadata": {
        "id": "CKptftZGoLhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize API\n",
        "api = HfApi()\n",
        "\n",
        "models = api.list_models(\n",
        "    task=\"text-generation\",\n",
        "    library=\"transformers\",\n",
        "    sort=\"downloads\",\n",
        "    direction=-1,\n",
        "    limit=50  # Get top 50\n",
        ")\n",
        "\n",
        "# Display as a list\n",
        "model_list = []\n",
        "for model in models:\n",
        "    model_list.append({\n",
        "        \"Model ID\": model.id,\n",
        "        \"Downloads\": model.downloads if hasattr(model, 'downloads') else 0,\n",
        "        \"Likes\": model.likes if hasattr(model, 'likes') else 0,\n",
        "        \"Tags\": \", \".join(model.tags[:3]) if model.tags else \"\"\n",
        "    })\n",
        "\n",
        "# Show as DataFrame\n",
        "df = pd.DataFrame(model_list)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cJL5oGdD84xN",
        "outputId": "19a19591-28c5-4321-e636-e849ada037c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in 'list_models': task, library. Will not be supported from version '1.0'.\n",
            "\n",
            "Use `filter` instead.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Model ID  Downloads  Likes  \\\n",
              "0                               openai-community/gpt2   11144960   3001   \n",
              "1                            Qwen/Qwen2.5-7B-Instruct    7897741    843   \n",
              "2                                     Qwen/Qwen3-0.6B    7261136    741   \n",
              "3                        Gensyn/Qwen2.5-0.5B-Instruct    6501589     26   \n",
              "4                    meta-llama/Llama-3.1-8B-Instruct    5234642   4840   \n",
              "5                                  openai/gpt-oss-20b    4769961   3819   \n",
              "6                       dphn/dolphin-2.9.1-yi-1.5-34b    4717677     43   \n",
              "7                                google/gemma-3-1b-it    4504785    677   \n",
              "8                  TinyLlama/TinyLlama-1.1B-Chat-v1.0    4360479   1437   \n",
              "9                           Qwen/Qwen3-Embedding-0.6B    4254861    692   \n",
              "10                         Qwen/Qwen2.5-1.5B-Instruct    4234680    532   \n",
              "11     trl-internal-testing/tiny-Qwen2ForCausalLM-2.5    4142606      1   \n",
              "12                                  facebook/opt-125m    4111604    222   \n",
              "13                   meta-llama/Llama-3.2-1B-Instruct    3861472   1136   \n",
              "14                           Qwen/Qwen2.5-3B-Instruct    3810769    322   \n",
              "15                                openai/gpt-oss-120b    3807313   4073   \n",
              "16                        Qwen/Qwen3-4B-Instruct-2507    3625776    429   \n",
              "17                             bigscience/bloomz-560m    3083333    129   \n",
              "18  context-labs/meta-llama-Llama-3.2-3B-Instruct-...    3077097      7   \n",
              "19                 mistralai/Mistral-7B-Instruct-v0.2    2684643   2995   \n",
              "20                   Qwen/Qwen3-Next-80B-A3B-Instruct    2582156    841   \n",
              "21                                      Qwen/Qwen3-8B    2374600    706   \n",
              "22                              distilbert/distilgpt2    2334670    590   \n",
              "23           deepseek-ai/DeepSeek-R1-Distill-Qwen-32B    2309866   1462   \n",
              "24                        inference-net/Schematron-3B    1975054     96   \n",
              "25                   meta-llama/Llama-3.2-3B-Instruct    1924223   1786   \n",
              "26                          petals-team/StableBeluga2    1828305     20   \n",
              "27                                     Qwen/Qwen3-32B    1803674    559   \n",
              "28                            meta-llama/Llama-3.2-1B    1744311   2138   \n",
              "29                                    Qwen/Qwen2.5-7B    1728673    236   \n",
              "30                                vikhyatk/moondream2    1681434   1322   \n",
              "31                         meta-llama/Meta-Llama-3-8B    1652344   6355   \n",
              "32                   microsoft/Phi-3-mini-4k-instruct    1582404   1316   \n",
              "33                         Qwen/Qwen2.5-0.5B-Instruct    1564761    382   \n",
              "34                             allenai/OLMo-2-0425-1B    1434466     65   \n",
              "35                           Qwen/Qwen3-Reranker-0.6B    1386123    247   \n",
              "36                        openai-community/gpt2-large    1336484    335   \n",
              "37                   Qwen/Qwen3-30B-A3B-Instruct-2507    1334568    635   \n",
              "38                                      Qwen/Qwen3-4B    1311679    427   \n",
              "39                                    Qwen/Qwen3-1.7B    1265785    300   \n",
              "40                                    google-t5/t5-3b    1229066     49   \n",
              "41                             rednote-hilab/dots.ocr    1171131   1098   \n",
              "42                          Qwen/Qwen2.5-14B-Instruct    1030116    279   \n",
              "43                meta-llama/Meta-Llama-3-8B-Instruct     989762   4247   \n",
              "44          deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B     910800   1367   \n",
              "45        unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit     896114     85   \n",
              "46                      Qwen/Qwen2.5-32B-Instruct-AWQ     837188     87   \n",
              "47                  meta-llama/Llama-3.3-70B-Instruct     810594   2547   \n",
              "48                  meta-llama/Llama-3.1-70B-Instruct     794918    855   \n",
              "49                            Qwen/Qwen3-Embedding-8B     753994    414   \n",
              "\n",
              "                                         Tags  \n",
              "0                   transformers, pytorch, tf  \n",
              "1            transformers, safetensors, qwen2  \n",
              "2            transformers, safetensors, qwen3  \n",
              "3            transformers, safetensors, qwen2  \n",
              "4            transformers, safetensors, llama  \n",
              "5          transformers, safetensors, gpt_oss  \n",
              "6            transformers, safetensors, llama  \n",
              "7      transformers, safetensors, gemma3_text  \n",
              "8            transformers, safetensors, llama  \n",
              "9   sentence-transformers, safetensors, qwen3  \n",
              "10           transformers, safetensors, qwen2  \n",
              "11           transformers, safetensors, qwen2  \n",
              "12                  transformers, pytorch, tf  \n",
              "13           transformers, safetensors, llama  \n",
              "14           transformers, safetensors, qwen2  \n",
              "15         transformers, safetensors, gpt_oss  \n",
              "16           transformers, safetensors, qwen3  \n",
              "17         transformers, pytorch, tensorboard  \n",
              "18           transformers, safetensors, llama  \n",
              "19         transformers, pytorch, safetensors  \n",
              "20      transformers, safetensors, qwen3_next  \n",
              "21           transformers, safetensors, qwen3  \n",
              "22                  transformers, pytorch, tf  \n",
              "23           transformers, safetensors, qwen2  \n",
              "24           transformers, safetensors, llama  \n",
              "25           transformers, safetensors, llama  \n",
              "26           transformers, safetensors, llama  \n",
              "27           transformers, safetensors, qwen3  \n",
              "28           transformers, safetensors, llama  \n",
              "29           transformers, safetensors, qwen2  \n",
              "30      transformers, safetensors, moondream1  \n",
              "31           transformers, safetensors, llama  \n",
              "32            transformers, safetensors, phi3  \n",
              "33           transformers, safetensors, qwen2  \n",
              "34           transformers, safetensors, olmo2  \n",
              "35           transformers, safetensors, qwen3  \n",
              "36                  transformers, pytorch, tf  \n",
              "37       transformers, safetensors, qwen3_moe  \n",
              "38           transformers, safetensors, qwen3  \n",
              "39           transformers, safetensors, qwen3  \n",
              "40                  transformers, pytorch, tf  \n",
              "41     dots_ocr, safetensors, text-generation  \n",
              "42           transformers, safetensors, qwen2  \n",
              "43           transformers, safetensors, llama  \n",
              "44           transformers, safetensors, qwen2  \n",
              "45           transformers, safetensors, llama  \n",
              "46           transformers, safetensors, qwen2  \n",
              "47           transformers, safetensors, llama  \n",
              "48           transformers, safetensors, llama  \n",
              "49  sentence-transformers, safetensors, qwen3  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-90a4284e-8d39-418c-9638-45564538999b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model ID</th>\n",
              "      <th>Downloads</th>\n",
              "      <th>Likes</th>\n",
              "      <th>Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>openai-community/gpt2</td>\n",
              "      <td>11144960</td>\n",
              "      <td>3001</td>\n",
              "      <td>transformers, pytorch, tf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Qwen/Qwen2.5-7B-Instruct</td>\n",
              "      <td>7897741</td>\n",
              "      <td>843</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Qwen/Qwen3-0.6B</td>\n",
              "      <td>7261136</td>\n",
              "      <td>741</td>\n",
              "      <td>transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gensyn/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>6501589</td>\n",
              "      <td>26</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>meta-llama/Llama-3.1-8B-Instruct</td>\n",
              "      <td>5234642</td>\n",
              "      <td>4840</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>openai/gpt-oss-20b</td>\n",
              "      <td>4769961</td>\n",
              "      <td>3819</td>\n",
              "      <td>transformers, safetensors, gpt_oss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>dphn/dolphin-2.9.1-yi-1.5-34b</td>\n",
              "      <td>4717677</td>\n",
              "      <td>43</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>google/gemma-3-1b-it</td>\n",
              "      <td>4504785</td>\n",
              "      <td>677</td>\n",
              "      <td>transformers, safetensors, gemma3_text</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>TinyLlama/TinyLlama-1.1B-Chat-v1.0</td>\n",
              "      <td>4360479</td>\n",
              "      <td>1437</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Qwen/Qwen3-Embedding-0.6B</td>\n",
              "      <td>4254861</td>\n",
              "      <td>692</td>\n",
              "      <td>sentence-transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Qwen/Qwen2.5-1.5B-Instruct</td>\n",
              "      <td>4234680</td>\n",
              "      <td>532</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>trl-internal-testing/tiny-Qwen2ForCausalLM-2.5</td>\n",
              "      <td>4142606</td>\n",
              "      <td>1</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>facebook/opt-125m</td>\n",
              "      <td>4111604</td>\n",
              "      <td>222</td>\n",
              "      <td>transformers, pytorch, tf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>meta-llama/Llama-3.2-1B-Instruct</td>\n",
              "      <td>3861472</td>\n",
              "      <td>1136</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Qwen/Qwen2.5-3B-Instruct</td>\n",
              "      <td>3810769</td>\n",
              "      <td>322</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>openai/gpt-oss-120b</td>\n",
              "      <td>3807313</td>\n",
              "      <td>4073</td>\n",
              "      <td>transformers, safetensors, gpt_oss</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
              "      <td>3625776</td>\n",
              "      <td>429</td>\n",
              "      <td>transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>bigscience/bloomz-560m</td>\n",
              "      <td>3083333</td>\n",
              "      <td>129</td>\n",
              "      <td>transformers, pytorch, tensorboard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>context-labs/meta-llama-Llama-3.2-3B-Instruct-...</td>\n",
              "      <td>3077097</td>\n",
              "      <td>7</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>mistralai/Mistral-7B-Instruct-v0.2</td>\n",
              "      <td>2684643</td>\n",
              "      <td>2995</td>\n",
              "      <td>transformers, pytorch, safetensors</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Qwen/Qwen3-Next-80B-A3B-Instruct</td>\n",
              "      <td>2582156</td>\n",
              "      <td>841</td>\n",
              "      <td>transformers, safetensors, qwen3_next</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Qwen/Qwen3-8B</td>\n",
              "      <td>2374600</td>\n",
              "      <td>706</td>\n",
              "      <td>transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>distilbert/distilgpt2</td>\n",
              "      <td>2334670</td>\n",
              "      <td>590</td>\n",
              "      <td>transformers, pytorch, tf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>deepseek-ai/DeepSeek-R1-Distill-Qwen-32B</td>\n",
              "      <td>2309866</td>\n",
              "      <td>1462</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>inference-net/Schematron-3B</td>\n",
              "      <td>1975054</td>\n",
              "      <td>96</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
              "      <td>1924223</td>\n",
              "      <td>1786</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>petals-team/StableBeluga2</td>\n",
              "      <td>1828305</td>\n",
              "      <td>20</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Qwen/Qwen3-32B</td>\n",
              "      <td>1803674</td>\n",
              "      <td>559</td>\n",
              "      <td>transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>meta-llama/Llama-3.2-1B</td>\n",
              "      <td>1744311</td>\n",
              "      <td>2138</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Qwen/Qwen2.5-7B</td>\n",
              "      <td>1728673</td>\n",
              "      <td>236</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>vikhyatk/moondream2</td>\n",
              "      <td>1681434</td>\n",
              "      <td>1322</td>\n",
              "      <td>transformers, safetensors, moondream1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>meta-llama/Meta-Llama-3-8B</td>\n",
              "      <td>1652344</td>\n",
              "      <td>6355</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>microsoft/Phi-3-mini-4k-instruct</td>\n",
              "      <td>1582404</td>\n",
              "      <td>1316</td>\n",
              "      <td>transformers, safetensors, phi3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>1564761</td>\n",
              "      <td>382</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>allenai/OLMo-2-0425-1B</td>\n",
              "      <td>1434466</td>\n",
              "      <td>65</td>\n",
              "      <td>transformers, safetensors, olmo2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Qwen/Qwen3-Reranker-0.6B</td>\n",
              "      <td>1386123</td>\n",
              "      <td>247</td>\n",
              "      <td>transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>openai-community/gpt2-large</td>\n",
              "      <td>1336484</td>\n",
              "      <td>335</td>\n",
              "      <td>transformers, pytorch, tf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Qwen/Qwen3-30B-A3B-Instruct-2507</td>\n",
              "      <td>1334568</td>\n",
              "      <td>635</td>\n",
              "      <td>transformers, safetensors, qwen3_moe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Qwen/Qwen3-4B</td>\n",
              "      <td>1311679</td>\n",
              "      <td>427</td>\n",
              "      <td>transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Qwen/Qwen3-1.7B</td>\n",
              "      <td>1265785</td>\n",
              "      <td>300</td>\n",
              "      <td>transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>google-t5/t5-3b</td>\n",
              "      <td>1229066</td>\n",
              "      <td>49</td>\n",
              "      <td>transformers, pytorch, tf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>rednote-hilab/dots.ocr</td>\n",
              "      <td>1171131</td>\n",
              "      <td>1098</td>\n",
              "      <td>dots_ocr, safetensors, text-generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Qwen/Qwen2.5-14B-Instruct</td>\n",
              "      <td>1030116</td>\n",
              "      <td>279</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>meta-llama/Meta-Llama-3-8B-Instruct</td>\n",
              "      <td>989762</td>\n",
              "      <td>4247</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B</td>\n",
              "      <td>910800</td>\n",
              "      <td>1367</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit</td>\n",
              "      <td>896114</td>\n",
              "      <td>85</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>Qwen/Qwen2.5-32B-Instruct-AWQ</td>\n",
              "      <td>837188</td>\n",
              "      <td>87</td>\n",
              "      <td>transformers, safetensors, qwen2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>meta-llama/Llama-3.3-70B-Instruct</td>\n",
              "      <td>810594</td>\n",
              "      <td>2547</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>meta-llama/Llama-3.1-70B-Instruct</td>\n",
              "      <td>794918</td>\n",
              "      <td>855</td>\n",
              "      <td>transformers, safetensors, llama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>Qwen/Qwen3-Embedding-8B</td>\n",
              "      <td>753994</td>\n",
              "      <td>414</td>\n",
              "      <td>sentence-transformers, safetensors, qwen3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-90a4284e-8d39-418c-9638-45564538999b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-90a4284e-8d39-418c-9638-45564538999b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-90a4284e-8d39-418c-9638-45564538999b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0968792d-e953-495d-a51a-c4e7e8615f62\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0968792d-e953-495d-a51a-c4e7e8615f62')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0968792d-e953-495d-a51a-c4e7e8615f62 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_765ff41f-99d4-410f-afba-f58666fdb7d6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_765ff41f-99d4-410f-afba-f58666fdb7d6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50,\n  \"fields\": [\n    {\n      \"column\": \"Model ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"meta-llama/Llama-3.2-1B-Instruct\",\n          \"Qwen/Qwen3-1.7B\",\n          \"vikhyatk/moondream2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Downloads\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2114503,\n        \"min\": 753994,\n        \"max\": 11144960,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          3861472,\n          1265785,\n          1681434\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Likes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1425,\n        \"min\": 1,\n        \"max\": 6355,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          1136,\n          300,\n          1322\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tags\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"transformers, safetensors, qwen3_next\",\n          \"transformers, safetensors, phi3\",\n          \"transformers, pytorch, tf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Making Your First API Call**\n",
        "\n",
        "In this section, youâ€™ll learn how to use the Hugging Face Inference API to send requests. For text tasks, youâ€™ll use endpoints designed for text generation or analysis. For image tasks, youâ€™ll use models specifically for image generation or classification(which we would cover later in this course).\n",
        "\n",
        "When interacting with chat-based models through Hugging Faceâ€™s `InferenceClient`, a common endpoint is the **chat completions** interface for instruction-following models.\n",
        "\n",
        "#### **Chat Completions API Overview**\n",
        "\n",
        "The chat-style API allows both single-turn and multi-turn interactions by processing a sequence of messages and generating coherent responses. It works well for both conversations and one-off queries.\n",
        "\n",
        "#### **Input Structure & Parameters**\n",
        "\n",
        "**A. Messages**\n",
        "You send a list of messages structured like:\n",
        "\n",
        "```python\n",
        "[\n",
        "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "  {\"role\": \"user\", \"content\": \"Tell me about Yoruba culture.\"}\n",
        "]\n",
        "```\n",
        "\n",
        "Each message contains:\n",
        "\n",
        "* **role**: Either `\"system\"`, `\"user\"`, or `\"assistant\"`\n",
        "* **content**: The actual message string\n",
        "\n",
        "**B. Max Tokens**\n",
        "Controls how much text the model should generate.\n",
        "\n",
        "```python\n",
        "response = client.chat_completion(\n",
        "    model=\"meta-llama/Llama-3-8B-Instruct\",\n",
        "    messages=messages,\n",
        "    max_tokens=150\n",
        ")\n",
        "```\n",
        "\n",
        "Tokens are subword units, not full words. For example, â€œunexpectedlyâ€ might tokenize to:\n",
        "`[\"un\", \"expect\", \"ed\", \"ly\"]`\n",
        "\n",
        "Each model has its own token limit. For example, Llama 3 (8B) supports up to ~8,192 tokens (input + output combined).\n",
        "\n",
        "**C. Temperature**\n",
        "Controls the randomness of the output. A lower temperature leads to more predictable, deterministic text; a higher temperature increases creativity and variety but may also increase the risk of irrelevant or nonsensical responses. The value ranges between 0 and 2:\n",
        "\n",
        "* `temperature = 0.0`: More deterministic\n",
        "* `temperature = 2.0`: More diverse and creative\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "response = client.chat_completion(\n",
        "    messages=messages,\n",
        "    temperature=0.7\n",
        ")\n",
        "```\n",
        "\n",
        "Higher values produce more varied results, lower values are better for accuracy and repetition control."
      ],
      "metadata": {
        "id": "JNBy4HsRoZTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define user Prompt\n",
        "prompt=\"Recommend to me a very interesting movie.\"\n",
        "# Creating a message as required by the API\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "completion = client.chat_completion(\n",
        "    messages = messages,\n",
        "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    max_tokens=300,\n",
        "    temperature=1.0,\n",
        ")\n",
        "print(completion)\n",
        "Markdown(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "_g26Y5-noZlY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "2d18a0b6-63e5-4b82-d936-b7d22004f72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionOutput(choices=[ChatCompletionOutputComplete(finish_reason='length', index=0, message=ChatCompletionOutputMessage(role='assistant', content='I\\'d like to recommend a thought-provoking and visually stunning movie that explores the concept of reality, identity, and memory.\\n\\n**Movie: \"Eternal Sunshine of the Spotless Mind\" (2004)**\\n\\nDirected by Michel Gondry, this film tells the story of a couple, Joel (Jim Carrey) and Clementine (Kate Winslet), who undergo a procedure to erase their memories of each other after a painful breakup. The movie then follows Joel as he undergoes the procedure, reliving memories of his time with Clementine and experiencing the fragmentation of their relationship.\\n\\n**Why you\\'ll find it interesting:**\\n\\n1. **Unique narrative structure**: The film\\'s non-linear storytelling and unconventional use of flashbacks and dream sequences make it a fascinating and intellectually engaging watch.\\n2. **Exploration of love and heartbreak**: The movie poignantly captures the complexities of human emotions, particularly the intensity and depth of love, as well as the pain and longing that can follow a breakup.\\n3. **Michel Gondry\\'s visual storytelling**: The film\\'s cinematography, production design, and special effects create a dreamlike atmosphere, immersing the viewer in the characters\\' emotional experiences.\\n\\n**Eternal Sunshine of the Spotless Mind** is a thought-provoking, beautiful, and emotionally resonant movie that will leave you reflecting on the nature of love, memory, and the human experience.\\n\\n(Warning: The movie deals with mature themes, including memory loss', reasoning=None, tool_call_id=None, tool_calls=None), logprobs=None, content_filter_results={'hate': {'filtered': False}, 'self_harm': {'filtered': False}, 'sexual': {'filtered': False}, 'violence': {'filtered': False}, 'jailbreak': {'filtered': False, 'detected': False}, 'profanity': {'filtered': False, 'detected': False}})], created=1761765643, id='chatcmpl-6e5dc56499df4dfa949e03ce541596a2', model='meta-llama/llama-3.1-8b-instruct', system_fingerprint='', usage=ChatCompletionOutputUsage(completion_tokens=300, prompt_tokens=43, total_tokens=343, prompt_tokens_details=None, completion_tokens_details=None), object='chat.completion')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I'd like to recommend a thought-provoking and visually stunning movie that explores the concept of reality, identity, and memory.\n\n**Movie: \"Eternal Sunshine of the Spotless Mind\" (2004)**\n\nDirected by Michel Gondry, this film tells the story of a couple, Joel (Jim Carrey) and Clementine (Kate Winslet), who undergo a procedure to erase their memories of each other after a painful breakup. The movie then follows Joel as he undergoes the procedure, reliving memories of his time with Clementine and experiencing the fragmentation of their relationship.\n\n**Why you'll find it interesting:**\n\n1. **Unique narrative structure**: The film's non-linear storytelling and unconventional use of flashbacks and dream sequences make it a fascinating and intellectually engaging watch.\n2. **Exploration of love and heartbreak**: The movie poignantly captures the complexities of human emotions, particularly the intensity and depth of love, as well as the pain and longing that can follow a breakup.\n3. **Michel Gondry's visual storytelling**: The film's cinematography, production design, and special effects create a dreamlike atmosphere, immersing the viewer in the characters' emotional experiences.\n\n**Eternal Sunshine of the Spotless Mind** is a thought-provoking, beautiful, and emotionally resonant movie that will leave you reflecting on the nature of love, memory, and the human experience.\n\n(Warning: The movie deals with mature themes, including memory loss"
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will now learn how to have a multi-turn conversation with this LLM. To do this, we will add the assistant's response to the previous conversation and also include the new prompt in the same message format. After that, we will provide a list of dictionaries to the chat completion function.\n",
        "\n",
        "#### **Conversation Dynamics**\n",
        "Conversations can vary in length from a single message to a series of exchanges. Typically, these interactions might start with a system message to guide the assistant's behavior, followed by a sequence of alternating messages between the user and the assistant.\n",
        "\n",
        "#### **Roles Explained**\n",
        "- **System**: The system message sets the initial tone or guidelines for the assistantâ€™s behavior during the interaction. It can be used to imbue the assistant with a specific personality or to provide precise instructions on how it should conduct itself. While the system message is optional, its absence defaults the assistant's demeanor to that of a generally helpful nature, akin to starting with a message like \"You are a helpful assistant.\"\n",
        "  \n",
        "- **User**: Messages from the user generally consist of queries or comments that prompt responses from the assistant. These are the driving force of the conversation, guiding the topics and flow of the dialogue.\n",
        "\n",
        "- **Assistant**: This role involves messages generated in response to the user or system inputs. The assistant's messages can include responses based on previous interactions within the conversation. Alternatively, you can manually craft messages in this role to demonstrate preferred responses or to simulate typical interactions.\n",
        "\n",
        "By understanding and effectively utilizing these roles, you can create nuanced and dynamic dialogues tailored to specific interaction scenarios or conversational needs."
      ],
      "metadata": {
        "id": "C-GYbIFopSEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-turn conversation with system message\n",
        "response = client.chat_completion(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the FIFA world in 2022?\"},\n",
        "        {\"role\": \"assistant\", \"content\": \"The Argentina won the FIFA world cup in 2022.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "fM4YKupWoZoa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "7009188e-3ccb-46ec-d5ff-61ce040c2cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " The 2022 FIFA World Cup was played in Qatar."
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Gradio\n",
        "\n",
        "[Gradio](https://www.gradio.app/docs) is an openâ€‘source Python library that makes it easy to build a webâ€‘based interface around any Python function, model, or API. With just a few lines of code you can wrap your machineâ€‘learning model (or any processing function) into a usable UI and launch it locally or share it publicly. Gradio abstracts away the need for frontâ€‘end web development skills, enabling nonâ€‘technical users to interact with your model via browser input fields, sliders, image uploads, etc. Once your interface is running, you can also host it on platforms like Hugging Face Spaces so others can try it from anywhere.\n"
      ],
      "metadata": {
        "id": "QUqFTIPnTosN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet"
      ],
      "metadata": {
        "id": "dGN4xZB1CgsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import libraries\n",
        "import gradio as gr\n",
        "import os"
      ],
      "metadata": {
        "id": "0jCatk_sDO9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_model(prompt):\n",
        "    try:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\",   \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        response = client.chat_completion(\n",
        "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "            messages=messages,\n",
        "            max_tokens=500\n",
        "        )\n",
        "\n",
        "        print(response.choices[0].message.content)\n",
        "        print(response.usage.prompt_tokens)\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\""
      ],
      "metadata": {
        "id": "4Oh3FPbeDQsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Create a Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=chat_with_model,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask me anything...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Chat with AI Models\",\n",
        "    description=\"Ask the model any questions.\"\n",
        ")"
      ],
      "metadata": {
        "id": "BauzL2nCDt83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "EtBGpcgiD0qj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "outputId": "5467cfe5-0472-4696-ceb6-1fdf5d28d232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://419edbf9b3588c37d2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://419edbf9b3588c37d2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### **Assignment: Add Memory to Your Chatbot**\n",
        "\n",
        "### Your Task\n",
        "Modify the chatbot function above, so it remembers previous messages in the conversation.\n",
        "\n",
        "### Current Problem\n",
        "Right now, your chatbot forgets everything after each response. If you say \"My name is Sarah\" and then ask \"What's my name?\", it won't remember.\n",
        "\n",
        "### What You Need to Do\n",
        "Make your chatbot remember all previous messages and responses, so it can refer back to earlier parts of the conversation.\n",
        "\n",
        "### Test Your Memory\n",
        "Your chatbot should be able to handle this conversation:\n",
        "```\n",
        "User: \"Hi, my name is Sarah and I love pizza.\"\n",
        "Bot: [responds]\n",
        "User: \"What's my name?\"\n",
        "Bot: [should say \"Sarah\"]\n",
        "User: \"What do I love?\"\n",
        "Bot: [should say \"pizza\"]\n",
        "```"
      ],
      "metadata": {
        "id": "Srt4_RvMTjbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hint**\n",
        "\n",
        "To give the chatbot a memory, youâ€™ll need to keep *all* of the past messages (user + bot) in a list of dictionaries behind the scenes, and then include that history every time you make a new API call. Hereâ€™s stepâ€‘byâ€‘step how you can do it:\n",
        "\n",
        "1. At the top of your script (outside the function) create a variable, e.g.\n",
        "\n",
        "   ```python\n",
        "   chat_history = []\n",
        "   ```\n",
        "\n",
        "   This will hold the sequence of all messages.\n",
        "\n",
        "2. Each time the user sends a prompt, add a dictionary representing the user message to `chat_history`, e.g.\n",
        "\n",
        "   ```python\n",
        "   chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "   ```\n",
        "\n",
        "3. Then when you call the model, pass *all* of the previous messages + the current user message as the `messages` list. For example:\n",
        "\n",
        "   ```python\n",
        "   messages = chat_history.copy()\n",
        "   messages.append({\"role\": \"assistant\", \"content\": ???})  # youâ€™ll do this after you get the response\n",
        "   ```\n",
        "\n",
        "4. After the model returns a response, take the assistantâ€™s reply content and append another dictionary into `chat_history`:\n",
        "\n",
        "   ```python\n",
        "   chat_history.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "   ```\n",
        "\n",
        "5. That way, when you next ask â€œWhatâ€™s my name?â€ or â€œWhat do I love?â€, the history contains the earlier statement (â€œMy name is Sarah and I love pizza.â€) and then the question follows. The model sees the entire chain and can answer accordingly.\n",
        "\n",
        "6. If you want, you can limit how many past messages you keep (for token/efficiency reasons) by slicing the list (e.g., `chat_history = chat_history[-10:]`).\n",
        "\n",
        "By following those steps, your chatbot will â€œrememberâ€ previous messages in the conversation."
      ],
      "metadata": {
        "id": "Z3lPefxCMOd-"
      }
    }
  ]
}